# 标题：《DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning》
## 基本信息：
设计了一个适用于大规模CNN和DNN的加速器，强调了内存对加速器设计、性能和能量的影响。证明了设计一种高吞吐量的加速器是可能的，在面积为3.02mm2和485 mW中能够执行452GOP/s（关键的神经网络操作，如突触权重乘法和神经元输出加法），与128位2 GHz SIMD处理器相比，该加速器的速度提高了117.87倍，总功耗降低了21.08倍。在65 nm处布局后，获得了加速器的特性。在很小的空间内获得如此高的吞吐量，可以在广泛的系统和广泛的应用程序中使用最先进的机器学习算法。
## 总体结构：
加速器主要组件：输入神经元的输入缓冲器(NBIN)，输出神经元的输出缓冲器(NBOUT)，以及第三个突触权重缓冲器(SB)，连接到神经功能单元(NFU)的计算块(执行突触和神经元计算)和控制逻辑(CP)。将存储拆分为专用结构，第一个好处是将SRAM调整为适当的读/写宽度，可以为每个读取请求提供最佳的时间和精力；第二个好处是避免冲突，允许完全消除数据冲突。
·使用双端口SRAM；
·在NFU-2中引入了专用寄存器来存储部分和；
·将Tn部分和轮换到NBout来将其用作临时存储缓冲区（NBout不仅连接到NFU-3和内存，还连接到NFU-2）；
加速器的控制：控制处理器(CP)驱动三个缓冲器和NFU的DMA的执行。
·输入神经元在缓冲器中被旋转以用于下一块Tn神经元（8次）；
·NFU-2的输入是RESET；
最后，当发送最后一块输入神经元(表中的最后一条指令)时，NBout的(存储)DMA设置为写入512字节 (256个输出) ，NBout指令为STORE；下一条指令的NBout写入操作将为NOP (在第一个块设置DMA，并自动将数据存储回内存，直到DMA结束)。

# 标题：《DaDianNao: A Machine-Learning Supercomputer》
## 总体结构：
①完全分布式的，没有主内存的体系结构：突触总是存储在使用它们的神经元附近，最大限度地减少了数据移动，节省了时间和精力；
②不对称的体系结构，其中每个节点的足迹在很大程度上偏向于存储而不是计算；
③传输神经元的值而不是突触的值；
④将本地存储分成许多块来实现高内部带宽。
## 基本设计特征：
将突触的存储定位在靠近神经元的位置，并使其变得庞大。设计选择的动机是决定只移动神经元，并将突触保存在固定的存储位置，两个原因：
①由于存在的突触比神经元多得多，所以移动神经元输出而不是突触是唯一合乎逻辑的做法；
②将所有突触(大多数计算输入)都放在计算运算符旁边，可以提供低能量/低延迟的数据(突触)传输和高内部带宽。
·提供足够的eDRAM容量来容纳所有芯片的组合eDRAM上的所有突触，将节省片外DRAM访问。为了弥补eDRAM和SRAM相比之下的缺点（延迟更长、破坏性读取和周期性刷新），将eDRAM分成四个库，并在四个库中交织突触行。
·所有的瓦片都通过一棵胖子树连接起来，这棵树用来向每个瓦片广播输入神经元值，并收集每个瓦片的输出神经元值。在芯片的中心，有两个特殊的eDRAM组：一个用于输入神经元，另一个用于输出神经元。每组输入神经元的中间值被局部保存在瓦片eDRAM中。当输出神经元的计算完成时，该值通过树被发送到芯片的中心，并被发送到相应的中央eDRAM库。
·16位运算符用于推理，32位运算符用于训练（训练不用16位运算符为了降低定点误差）。
·使用简单的2D网格拓扑。
·在磁贴的中心模块旁边，实现了路由器，有四个流水线阶段：路由计算(RC)、虚电路分配(VA)、交换机分配(SA)和交换机遍历(ST)。
·控件提供两种操作模式：一次处理一行或批量学习：针对不同的输入数据，同时处理多行。作用：实现更稳定的梯度下降，而且还具有改善突触重用的好处，但由于必须要存储多个输入/输出实例，会导致收敛速度较慢，存储容量较大。
·通信，有三个主要层面的情况需要考虑：
①卷积和汇聚层的特征是由用于采样输入神经元的小窗口(卷积或汇聚核)定义的局部连通性；
②由于本地连接，节点间通信量非常低(大多数通信是节点内的)，大部分发生在映射到每个节点的层矩形的边界。
③局部响应归一化层，不存在节点通信。
·通信方案：一个节点一旦完成了自己的计算，就可以开始处理新到达的输入神经元块，并且已经发送了先前的输入神经元块。

# 标题：《PuDianNao: A Polyvalent Machine Learning Accelerator》
## 基本信息：
提出了一个称为普甸脑的ML加速器，它综合了七种具有代表性的ML技术，包括k-均值、k-近邻、naive bayes、支持向量机、线性回归、分类树和深度神经网络。通过对不同ML技术的计算基元和局部性的深入分析，普甸脑在3.51mm2的面积上可以执行高达1056GOP/s(例如加法和乘法)，而功耗仅为596 mW。与NVIDIA K20M GPU(28 nm制程)相比，普电脑制(65 nm制程)速度快1.20倍，能耗降低128.41倍。
## 总体结构：
PuDianNao由多个功能单元(FU)、三个数据缓冲区(HotBuf、ColdBuf和OutputBuf)、一个指令缓冲区(InstBuf)、一个控制模块和一个DMA组成：
·功能单元(FUS)是普甸脑ML加速器的基本执行单元，每个FU由机器学习功能单元(MLU)和算术逻辑单元(ALU)两部分组成；
·MLU被设计成支持几个在典型ML技术中常见的基本但重要的计算基元，包括点积(LR、SVM和DNN)、距离计算(k-NN和k-Means)、计数(ID3和NB)、排序(k-NN和k-Means)、非线性函数(例如，sigmoid和tanh)等。
为了降低普甸脑加速器的面积和功耗，我们在级加法器、乘法器和加法器树上实现了16位浮点运算单元。同时，其余三个阶段(计数器、ACC和Misc)实现32位浮点单元，以避免潜在的溢出。
·在每个FU中增加了一个小型算术逻辑单元(ALU)，它包含一个加法器、一个乘法器和一个除法器，以及32位浮点到16位浮点和16位浮点到32位浮点的转换器。此外，为了支持训练分类树所需的逻辑函数，使用算术逻辑单元对LOG(1−x)的泰勒展开进行近似计算。
·在普甸脑加速器中放置了三个独立的片上数据缓冲器：
HotBuf(8KB)、ColdBuf(16KB)和OutputBuf(8KB)。HotBuf存储重用距离较短的输入数据（存储测试实例，读取宽度为u×f×16bit），ColdBuf存储重用距离相对较长的输入数据。OutputBuf存储输出数据或临时结果。设置HotBuf和ColdBuf缓冲区来减少不同读取宽度带来的开销。
单端口SRAM：HotBuf和ColdBuf；
双端口SRAM: OutputBuf。
三个缓冲器都连接到同一DMA。

# 标题：《ShiDianNao: Shifting Vision Processing Closer to the Sensor》
## 基本信息：
利用CNN算法特性，研究了一种节能的视觉识别加速器的设计，它可以直接嵌入任何CMOS或CCD传感器，并且速度足够快，能够实时处理图像。65 nm的完整设计（比以前最先进的神经网络加速器节能60倍），占地面积为4.86mm2，功耗仅为320 mW，但仍比高端GPU快约30倍。
## 总体结构：
加速器由以下主要组件组成：两个用于输入和输出神经元的缓冲器(NBIN和NBOUT)，一个用于突触的缓冲器(SB)，一个神经功能单元(NFU)加上一个用于计算输出神经元的算术单元(ALU)，以及一个用于指令的缓冲器和解码器(IB)。有两个功能单元，一个是容纳基本神经元运算(乘法、加法和比较)的NFU，另一个是执行激活函数计算的ALU。使用16位定点运算。
·NFU神经功能单元：
由Px×PyProcessing Elements(PES)组成的2D网格，以处理2D数据(神经元/像素阵列)。NFU可以同时读取突触，并从NBIN/NBout和SB输入神经元，然后将它们分配到不同的PE。NFU将本地存储结构包含到每个PE中，这使得输入神经元能够在PE之间进行本地传播，执行计算后，NFU从不同PE收集结果，并将其发送到NBOUT/NBIN或ALU。
·PE
具有三个输入：
①一个输入用于接收控制信号；
②一个输入用于从SB读取突触(例如，卷积层的核值)；
③一个输入用于根据控制信号从NBIN/NBout、从Pei+1，j(右邻居)或从Pei+1，j(底部邻居)读取神经元。
有两个输出：
①一个输出用于将计算结果写入NBOUT/NBIN；
②另一个输出用于将本地存储的神经元传播到邻居PE（便于有效重用数据）。
在执行CNN层时，每个PE连续容纳单个输出神经元，并且只有在计算出当前输出神经元时才会切换到另一个输出神经元。
PE间数据传输：
通过在每个PE中有两个FIFO来实现允许PE间的数据在PE网格上传播，每个PE可以将本地存储的输入神经元发送到它的左侧和下部邻居，以临时存储它接收到的输入值。
·算数逻辑单元ALU
在ALU中实现了16位定点算术运算符，包括除法(用于平均池和归一化层)和非线性激活函数，如tanh()和sigmoid()(用于卷积层和池层)。使用分段线性插值来计算激活函数值。
·加速器存储架构
NBIN和NBOUT分别存储输入和输出神经元，并在计算完所有输出神经元并成为下一层的输入神经元时交换它们的功能。它们中的每一个都有2×Pybank，NBIN和NBOUT必须足够大以存储整个层的所有神经元。

# 标题：《Cambricon: An Instruction Set Architecture for Neural Networks》
## 基本信息：
一种用于神经网络加速器的新型ISA，Cambricon是一种加载-存储体系结构，其指令均为64位，包含64个用于标量的32位通用寄存器(GPR)，主要用于控制和寻址。Cambricon不使用任何矢量寄存器文件，而是将数据保存在程序员/编译器可见的片上便签式存储器中，而且不需要在片上存储器中实现多个端口。Cambricon有效地支持更大且可变的数据宽度（因为便签式存储器很轻松实现比寄存器堆更宽）。
1)提出了一种对神经网络技术具有很强描述能力的轻量级ISA算法；
2)对现有的神经网络技术的计算模式进行了全面的研究；
3)利用台积电65 nm技术实现了第一个基于Cambricon的加速器，对Cambricon算法的有效性进行了评估。
Cambricon的一个原型加速器包含七个主要的指令流水线阶段：读取、解码、发布、寄存器读取、执行、写回和交付。在这个加速器中使用了如便签式存储器和DMA技术。
## 总体流程：
在提取和解码阶段之后，指令被注入到有序发布队列中。从标量寄存器堆成功获取操作数后，将根据指令类型将指令发送到不同的单元。控制指令和标量计算/逻辑指令将被发送到标量功能单元以供直接执行。在写入标量寄存器文件之后，若该指令是已执行但是最早未提交的指令，就可以从重定向缓冲区将其提交。
可以访问L1高速缓存或便签存储器的数据传输指令、矢量计算指令和矢量逻辑指令将被发送到地址生成单元（需要在有序存储器队列中等待，以解决与存储器队列中较早指令的潜在存储器依赖性）。之后，标量数据传输指令的load/store请求将被发送到L1高速缓存，矢量的数据传输/计算/逻辑指令将被发送到矢量功能单元，矩阵的数据传输/计算指令将被发送到矩阵功能单元。指令如果是最早未提交的指令，也可以从存储器队列中引退，然后从重新排序缓冲器中交付。
该加速器实现了矢量和矩阵功能单元。该矢量单元包含32个16位加法器、32个16位乘法器，并配备64KB便签式存储器。矩阵单元包含1024个乘法器和1024个加法器，被分成32个独立的计算块。每个计算块都配有单独的24KB便签簿。这32个计算块通过用于向每个块广播输入值并从每个块收集输出值的H树总线连接。
## 总体结构特点：
它不使用任何矢量寄存器堆，而是将数据保存在片上便签式存储器中为了有效地访问便签存储器，原型加速器的矢量/矩阵功能单元集成了三个DMA，每个DMA对应于指令的一个矢量/矩阵输入/输出。便签式存储器还配备了IO DMA。
便签式存储器设计了一种特定的结构：根据地址的低位两位将存储器分解为四个存储体，并通过一个纵横线将它们连接到四个读写端口上，以保证不会有存储体被同时访问。

# 标题：《Cambricon-X: An Accelerator for Sparse Neural Networks》
## 基本信息：
提出了一种新的加速器Cambricon-X，以利用神经网络模型的稀疏性和不规则性来提高效率。所提出的加速器采用基于PE的体系结构，由多个处理单元(PE)和缓冲控制器(BC)组成。BC集成了一个高效的索引模块，用于从集中的神经元缓冲区中只选择需要的神经元，然后将这些神经元传输到带宽要求较低的连接PE。在接收到这样的神经元后，PE可以用本地存储的压缩突触进行有效的计算。此外，由于突触的不规则分布，多个PE可以异步工作，以获得更高的效率。在16个PES的情况下，该加速器可以在6.38mm2和954mWat 65 nm的尺寸下实现最高544GOP/s的速度。
## 总体结构：
由控制处理器（CP），缓冲控制器（BC），两个神经缓冲区（NBin和NBout），直接存储器访问模块（DMA）和计算单元（CU）组成。所提出的体系结构的一个关键特征是BC中的索引单元。总共有一个索引单元，每个对应一个PE，用于选择其必需的神经元。
在本设计中，我们使用16位定点算术单元，而不是传统的32位浮点单元（16位定点单元的硬件成本比32位浮点单元低得多，而精度损失可以忽略不计，数据总线的宽度将减少一半）。
## 计算单元：
计算单元设计用于高效计算具有多个PE的神经网络的核心运算，即向量乘加运算。PE的架构，由突触缓冲器(SB)和PE的神经网络功能单元(PEFU)组成。PEFU将局部SB的突触和BC的神经元作为输入，产生的输出神经元将被送回BC。
①PEFU主要用于神经网络中的乘法和加法运算。
②SB用于存储分布式突触，SB的设计过程中存在两个关键问题。第一种是确定SB的适当大小，第二种是组织SB的突触。
③缓冲控制器：用于将必要的神经元传输到PE，在PE上协调计算，并执行计算密集度较低的操作。
④BCFU：主要用于存储要由IM选择的神经元，且BCFU有很多单元，因此可以同时存储神经元。
⑤IM:用于索引具有不同稀疏度的稀疏神经网络所需神经元。在BC中设计了一个集中的索引模块，只将索引的神经元传输到PE（降低神经缓冲区和PEs之间的带宽要求）。
⑥CP设计用于高效、灵活地控制各种指令的执行。指令用于数据组织、执行协调和存储器访问等，并且它们存储在小指令缓冲器中。
⑦NB包括NBin和NBout，分别用于存储输入和输出神经元：输入神经元从NBin中选择，然后发送到所有PE进行计算，输出神经元在计算后收集到NBout。（我们的加速器实现中，NBIN和NBOUT都使用8KB）
⑧采用Fat-tree互连拓扑结构（在互连层级顶部附近提供更多数据链接，以连接BC和所有PEs，以提高它们之间数据移动的效率）。
⑨片外存储器和片上缓冲器(包括NBIN、NBOUT和SB)之间的数据通信通过直接存储器访问(DMA)实现。先将所需的突触分成块，而后内存访问端口将在短时间内一次仅分配给一个PE。

# 标题：《DianNao Family: Energy-Efficient Hardware Accelerators for Machine Learning》
## 基本信息：
介绍了为ML(特别是神经网络)设计的一系列硬件加速器(即点脑系列)，特别强调了内存对加速器设计、性能和能量的影响。结果表明，在多个具有代表性的神经网络层上，与GPU相比，加速比可以达到450.65倍，对于64芯片的大典脑系统(点脑族成员)，平均可以降低150.31x的能耗。
